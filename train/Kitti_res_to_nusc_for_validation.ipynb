{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from pyquaternion import Quaternion\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box\n",
    "from lyft_dataset_sdk.utils.geometry_utils import BoxVisibility, transform_matrix\n",
    "from lyft_dataset_sdk.utils.kitti import KittiDB\n",
    "from lyft_dataset_sdk.eval.detection.mAP_evaluation import Box3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_calib_file(filepath):\n",
    "        with open(filepath) as f:\n",
    "            lines = f.readlines()\n",
    "    \n",
    "        obj = lines[2].strip().split(' ')[1:]\n",
    "        P2 = np.array(obj, dtype=np.float32)\n",
    "        obj = lines[3].strip().split(' ')[1:]\n",
    "        P3 = np.array(obj, dtype=np.float32)\n",
    "        obj = lines[4].strip().split(' ')[1:]\n",
    "        R0 = np.array(obj, dtype=np.float32)\n",
    "        obj = lines[5].strip().split(' ')[1:]\n",
    "        Tr_velo_to_cam = np.array(obj, dtype=np.float32)\n",
    "\n",
    "        return {'P2': P2.reshape(3, 4),\n",
    "                'P3': P3.reshape(3, 4),\n",
    "                'R_rect': R0.reshape(3, 3),\n",
    "                'Tr_velo2cam': Tr_velo_to_cam.reshape(3, 4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "15991 instance,\n",
      "8 sensor,\n",
      "128 calibrated_sensor,\n",
      "149072 ego_pose,\n",
      "148 log,\n",
      "148 scene,\n",
      "18634 sample,\n",
      "149072 sample_data,\n",
      "539765 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 10.2 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 4.2 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "# load lyft test dataset\n",
    "level5data = LyftDataset(data_path='.', json_path='/home/bob/data/lyft_data/train_data', verbose=True)\n",
    "valid_tokens = [x.strip() for x in open('/home/bob/data/lyft2kitti/valid.txt').readlines()]\n",
    "train_df = pd.read_csv('/home/bob/data/lyft_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loda kitti pred boxes\n",
    "pred_folder = '/home/bob/data/lyft2kitti/training/label_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15dfc55aa52543bf8d9f1892ab932548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2772), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_box3ds = []\n",
    "pred_box2ds = []\n",
    "kitti_to_nu_lidar = Quaternion(axis=(0,0,1), angle=np.pi/2)\n",
    "for sample_token in tqdm_notebook(valid_tokens):\n",
    "    pred_file = os.path.join(pred_folder, \"{}.txt\".format(sample_token))\n",
    "    calib_path = pred_file.replace('label_2','calib')\n",
    "    calib = read_calib_file(calib_path)\n",
    "    sample = level5data.get('sample', sample_token)\n",
    "    lidar_token = sample['data']['LIDAR_TOP']\n",
    "    sd_record_lid = level5data.get('sample_data', lidar_token) # lidar sample data\n",
    "    cs_record_lid = level5data.get('calibrated_sensor', sd_record_lid['calibrated_sensor_token']) \n",
    "    ego_pose = level5data.get(\"ego_pose\", sd_record_lid[\"ego_pose_token\"])\n",
    "    with open(pred_file) as f:\n",
    "        for line in f:\n",
    "            # Parse this line into box information.\n",
    "            parsed_line = KittiDB.parse_label_line(line) # in cam corrd\n",
    "            center = parsed_line[\"xyz_camera\"]\n",
    "            wlh = parsed_line[\"wlh\"]\n",
    "            yaw_camera = parsed_line[\"yaw_camera\"]\n",
    "            name = parsed_line[\"name\"]\n",
    "            score = parsed_line[\"score\"]\n",
    "            # quat_box \n",
    "            quat_box = Quaternion(axis=(0, 1, 0), angle=yaw_camera) * Quaternion(axis=(1, 0, 0), angle=np.pi / 2)\n",
    "            # 1: box in camera coord\n",
    "            box = Box([0.0, 0.0, 0.0], wlh, quat_box, name=name, token = sample_token)\n",
    "            # 2: center definition difference\n",
    "            box.translate(center + np.array([0, -wlh[2] / 2, 0]))\n",
    "            # 3: transform from camera to lidar\n",
    "            box.rotate(Quaternion(matrix=calib['R_rect']).inverse)\n",
    "            box.translate(-calib['Tr_velo2cam'][:,3])\n",
    "            box.rotate(Quaternion(matrix=calib['Tr_velo2cam'][:,:3]).inverse)\n",
    "            # 4: Transform to nuScenes LIDAR coord system.\n",
    "            box.rotate(kitti_to_nu_lidar)\n",
    "            box.score = score\n",
    "            # 5: transform from lidar to ego\n",
    "            box.rotate(Quaternion(cs_record_lid['rotation']))\n",
    "            box.translate(np.array(cs_record_lid['translation']))\n",
    "            # 6: transform from ego to global\n",
    "            box.rotate(Quaternion(ego_pose['rotation']))\n",
    "            box.translate(np.array(ego_pose['translation']))\n",
    "            \n",
    "            ## to 3D box format\n",
    "            box3d = Box3D(\n",
    "                sample_token=sample_token,\n",
    "                translation=list(box.center),\n",
    "                size=list(box.wlh),\n",
    "                rotation=list(box.orientation.elements),\n",
    "                name=name,\n",
    "                score=score\n",
    "            )\n",
    "            pred_box2ds.append(box)\n",
    "            pred_box3ds.append(box3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.9999314881405521\n",
      "('animal', 1.0)\n",
      "('bicycle', 1.0)\n",
      "('bus', 1.0)\n",
      "('car', 1.0)\n",
      "('motorcycle', 1.0)\n",
      "('other_vehicle', 1.0)\n",
      "('pedestrian', 0.9994519051244173)\n",
      "('truck', 1.0)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "pred = [b.serialize() for b in pred_box3ds]\n",
    "with open(\"json/yolo/pred_gt.json\", \"w\") as f:\n",
    "    json.dump(pred, f)\n",
    "!python mAP_evaluation.py --pred_file \"json/yolo/pred_gt.json\" --gt_file \"json/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
