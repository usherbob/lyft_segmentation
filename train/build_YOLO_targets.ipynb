{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s /media/bob/data/lyft/train_images images\n",
    "!ln -s /media/bob/data/lyft/train_maps maps\n",
    "!ln -s /media/bob/data/lyft/train_lidar lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some hyperparameters we'll need to define for the system\n",
    "voxel_size = (0.2, 0.2, 1.0)\n",
    "z_offset = -2.0\n",
    "bev_shape = (672, 672, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "level5data = LyftDataset(data_path='.', json_path='/media/bob/data/lyft/train_data', verbose=False)\n",
    "classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \n",
    "           \"animal\", \"emergency_vehicle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [(level5data.get('sample', record['first_sample_token'])['timestamp'], record) for record in\n",
    "        level5data.scene]\n",
    "\n",
    "entries = []\n",
    "\n",
    "for start_time, record in sorted(records):\n",
    "    start_time = level5data.get('sample', record['first_sample_token'])['timestamp'] / 1000000\n",
    "\n",
    "    token = record['token']\n",
    "    name = record['name']\n",
    "    date = datetime.utcfromtimestamp(start_time)\n",
    "    host = \"-\".join(record['name'].split(\"-\")[:2])\n",
    "    first_sample_token = record[\"first_sample_token\"]\n",
    "\n",
    "    entries.append((host, name, date, token, first_sample_token))\n",
    "            \n",
    "df = pd.DataFrame(entries, columns=[\"host\", \"scene_name\", \"date\", \"scene_token\", \"first_sample_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host\n",
      "host-a004    42\n",
      "host-a005     1\n",
      "host-a006     3\n",
      "host-a007    26\n",
      "host-a008     5\n",
      "host-a009     9\n",
      "host-a011    51\n",
      "host-a012     2\n",
      "host-a015     6\n",
      "host-a017     3\n",
      "host-a101    20\n",
      "host-a102    12\n",
      "Name: scene_token, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "host_count_df = df.groupby(\"host\")['scene_token'].count()\n",
    "print(host_count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_hosts = [\"host-a008\", \"host-a009\",\"host-a012\", \"host-a015\",\"host-a017\" ]\n",
    "\n",
    "validation_df = df[df[\"host\"].isin(validation_hosts)]\n",
    "vi = validation_df.index\n",
    "train_df = df[~df.index.isin(vi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_border(img, point1, point2, point3, point4, line_length):\n",
    "\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "    x3, y3 = point3\n",
    "    x4, y4 = point4    \n",
    "\n",
    "    cv2.circle(img, (x1, y1), 3, (255, 0, 255), -1)    #-- top_left\n",
    "    cv2.circle(img, (x2, y2), 3, (255, 0, 255), -1)    #-- bottom-left\n",
    "    cv2.circle(img, (x3, y3), 3, (255, 0, 255), -1)    #-- top-right\n",
    "    cv2.circle(img, (x4, y4), 3, (255, 0, 255), -1)    #-- bottom-right\n",
    "\n",
    "    cv2.line(img, (x1, y1), (x1 , y1 + line_length), (0, 255, 0), 2)  #-- top-left\n",
    "    cv2.line(img, (x1, y1), (x1 + line_length , y1), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.line(img, (x2, y2), (x2 , y2 - line_length), (0, 255, 0), 2)  #-- bottom-left\n",
    "    cv2.line(img, (x2, y2), (x2 + line_length , y2), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.line(img, (x3, y3), (x3 - line_length, y3), (0, 255, 0), 2)  #-- top-right\n",
    "    cv2.line(img, (x3, y3), (x3, y3 + line_length), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.line(img, (x4, y4), (x4 , y4 - line_length), (0, 255, 0), 2)  #-- bottom-right\n",
    "    cv2.line(img, (x4, y4), (x4 - line_length , y4), (0, 255, 0), 2)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformation_matrix_to_voxel_space(shape, voxel_size, offset):\n",
    "    \"\"\"\n",
    "    Constructs a transformation matrix given an output voxel shape such that (0,0,0) ends up in the center.\n",
    "    Voxel_size defines how large every voxel is in world coordinate, (1,1,1) would be the same as Minecraft voxels.\n",
    "    \n",
    "    An offset per axis in world coordinates (metric) can be provided, this is useful for Z (up-down) in lidar points.\n",
    "    No rotation, only a scale and translation.\n",
    "    \"\"\"\n",
    "    \n",
    "    shape, voxel_size, offset = np.array(shape), np.array(voxel_size), np.array(offset)\n",
    "    \n",
    "    tm = np.eye(4, dtype=np.float32) #(4,4)\n",
    "    translation = shape/2 + offset/voxel_size #(3,)\n",
    "    \n",
    "    tm = tm * np.array(np.hstack((1/voxel_size, [1]))) # (4,4)\n",
    "\n",
    "    tm[:3, 3] = np.transpose(translation)\n",
    "    return tm\n",
    "\n",
    "def transform_points(points, transf_matrix):\n",
    "    \"\"\"\n",
    "    Transform (3,N) or (4,N) points using transformation matrix.\n",
    "    \"\"\"\n",
    "    if points.shape[0] not in [3,4]:\n",
    "        raise Exception(\"Points input should be (3,N) or (4,N) shape, received {}\".format(points.shape))\n",
    "    return transf_matrix.dot(np.vstack((points[:3, :], np.ones(points.shape[1]))))[:3, :]\n",
    "\n",
    "def car_to_voxel_coords(points, shape, voxel_size, z_offset=0):\n",
    "    if len(shape) != 3:\n",
    "        raise Exception(\"Voxel volume shape should be 3 dimensions (x,y,z)\")\n",
    "        \n",
    "    if len(points.shape) != 2 or points.shape[0] not in [3, 4]:\n",
    "        raise Exception(\"Input points should be (3,N) or (4,N) in shape, found {}\".format(points.shape))\n",
    "\n",
    "    tm = create_transformation_matrix_to_voxel_space(shape, voxel_size, (0, 0, z_offset))\n",
    "    p = transform_points(points, tm) #(3, N)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_boxes_to_car_space(boxes, ego_pose):\n",
    "    \"\"\"\n",
    "    Move boxes from world space to car space.\n",
    "    Note: mutates input boxes.\n",
    "    \"\"\"\n",
    "    translation = -np.array(ego_pose['translation'])\n",
    "    rotation = Quaternion(ego_pose['rotation']).inverse\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Bring box to car space\n",
    "        box.translate(translation)\n",
    "        box.rotate(rotation)\n",
    "        \n",
    "def scale_boxes(boxes, factor):\n",
    "    \"\"\"\n",
    "    Note: mutates input boxes\n",
    "    \"\"\"\n",
    "    for box in boxes:\n",
    "        box.wlh = box.wlh * factor\n",
    "\n",
    "def draw_boxes(im, voxel_size, boxes, classes, z_offset=0.0):\n",
    "    for box in boxes:\n",
    "        # We only care about the bottom corners\n",
    "        corners = box.bottom_corners()\n",
    "        corners_voxel = car_to_voxel_coords(corners, im.shape, voxel_size, z_offset).transpose(1,0)\n",
    "        corners_voxel = corners_voxel[:,:2] # Drop z coord\n",
    "        #vertex = np.int0(corners_voxel[0,:]).reshape(2)\n",
    "        #print(vertex.shape)\n",
    "\n",
    "        #class_color = classes.index(box.name) + 1\n",
    "        \n",
    "        #if class_color == 0:\n",
    "        #    raise Exception(\"Unknown class: {}\".format(box.name))\n",
    "\n",
    "        im = draw_border(im, np.int0(corners_voxel[0,:]), np.int0(corners_voxel[1,:]), \n",
    "                      np.int0(corners_voxel[2,:]), np.int0(corners_voxel[3,:]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample_token = train_df.first_sample_token.values[1]\n",
    "sample = level5data.get(\"sample\", my_sample_token)\n",
    "sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "\n",
    "ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "calibrated_sensor = level5data.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "boxes = level5data.get_boxes(sample_lidar_token)\n",
    "move_boxes_to_car_space(boxes, ego_pose)\n",
    "im = cv2.imread('/media/bob/lyft/data/lyft_bev672/lyft_bev/bev_train_data/{}_input.png'.format(my_sample_token))\n",
    "\n",
    "draw_boxes(im, voxel_size, boxes, classes, z_offset)\n",
    "cv2.imshow('bev',im)\n",
    "k = cv2.waitKey(0)\n",
    "if k == 27:         # wait for ESC key to exit\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24b0962e44420e6322de3f25d9e4e5cc3c7a348ec00bfa69db21517e4ca92cc8'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sample_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1048.15595023, 1691.8102354 ,  -23.30494345])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes[0].center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.85948982, 48.68037288,  1.06134113])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_boxes_to_car_space(boxes, ego_pose)\n",
    "boxes[0].center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rotation': [-0.6004078747001647,\n",
       "  -0.000868287440477653,\n",
       "  0.0018651459228554272,\n",
       "  0.7996912850004297],\n",
       " 'translation': [1007.2332778546752, 1725.4217301399465, -24.58000073380586],\n",
       " 'token': '2d673d4bee560c77788b91e2ee24503538e74a23e7972e3e0099b92015f76dde',\n",
       " 'timestamp': 1557858039302414.8}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ego_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('/media/bob/data/lyft/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>24b0962e44420e6322de3f25d9e4e5cc3c7a348ec00bfa...</td>\n",
       "      <td>1048.155950230245 1691.8102354006162 -23.30494...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Id  \\\n",
       "11539  24b0962e44420e6322de3f25d9e4e5cc3c7a348ec00bfa...   \n",
       "\n",
       "                                        PredictionString  \n",
       "11539  1048.155950230245 1691.8102354006162 -23.30494...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df.Id==my_sample_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target_for_NN(first_sample_token, output_folder='./bev_target/'):\n",
    "    \"\"\"\n",
    "    Given a first sample token (in a scene), output rasterized input volumes and targets in birds-eye-view perspective.\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    sample_token = first_sample_token\n",
    "    \n",
    "    while sample_token:\n",
    "        \n",
    "        sample = level5data.get(\"sample\", sample_token)\n",
    "\n",
    "        sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "        lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "        lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "\n",
    "        ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "        boxes = level5data.get_boxes(sample_lidar_token)\n",
    "        move_boxes_to_car_space(boxes, ego_pose)\n",
    "        \n",
    "        label_path = self.label_folder.joinpath(f\"{sample_token}.txt\")\n",
    "        with open(label_path, \"w\") as label_file:\n",
    "            for box in boxes:\n",
    "                corners = box.bottom_corners()\n",
    "                corners_voxel = car_to_voxel_coords(corners, im.shape, voxel_size, z_offset).transpose(1,0)\n",
    "                corners_voxel = corners_voxel[:,:2]\n",
    "        \n",
    "        sample_token = sample[\"next\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3(torch_cu100)",
   "language": "python",
   "name": "torch_cu100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
