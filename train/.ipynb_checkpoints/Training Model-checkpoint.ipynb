{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS_FOLDER = \"./artifacts\"\n",
    "!rm -rf images\n",
    "!rm -rf maps\n",
    "!rm -rf lidar\n",
    "!ln -s /home/bob/data/lyft_data/train_images images\n",
    "!ln -s /home/bob/data/lyft_data/train_maps maps\n",
    "!ln -s /home/bob/data/lyft_data/train_lidar lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Disable multiprocesing for numpy/opencv. We already multiprocess ourselves, this would mean every subprocess produces\n",
    "# even more threads which would lead to a lot of context switching, slowing things down a lot.\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformation_matrix_to_voxel_space(shape, voxel_size, offset):\n",
    "    \"\"\"\n",
    "    Constructs a transformation matrix given an output voxel shape such that (0,0,0) ends up in the center.\n",
    "    Voxel_size defines how large every voxel is in world coordinate, \n",
    "    (1,1,1) would be the same as Minecraft voxels.\n",
    "    An offset per axis in world coordinates (metric) can be provided, \n",
    "    this is useful for Z (up-down) in lidar points.\n",
    "    No rotation, only a scale and translation.\n",
    "    \"\"\"\n",
    "    \n",
    "    shape, voxel_size, offset = np.array(shape), np.array(voxel_size), np.array(offset)\n",
    "    tm = np.eye(4, dtype=np.float32) #(4,4)\n",
    "    translation = shape/2 + offset/voxel_size #(3,)\n",
    "    tm = tm * np.array(np.hstack((1/voxel_size, [1]))) # (4,4)\n",
    "    tm[:3, 3] = np.transpose(translation)\n",
    "    return tm\n",
    "\n",
    "def transform_points(points, transf_matrix):\n",
    "    \"\"\"\n",
    "    Transform (3,N) or (4,N) points using transformation matrix.\n",
    "    \"\"\"\n",
    "    if points.shape[0] not in [3,4]:\n",
    "        raise Exception(\"Points input should be (3,N) or (4,N) shape, received {}\".format(points.shape))\n",
    "    return transf_matrix.dot(np.vstack((points[:3, :], np.ones(points.shape[1]))))[:3, :]\n",
    "\n",
    "def car_to_voxel_coords(points, shape, voxel_size, z_offset=0):\n",
    "    if len(shape) != 3:\n",
    "        raise Exception(\"Voxel volume shape should be 3 dimensions (x,y,z)\")\n",
    "        \n",
    "    if len(points.shape) != 2 or points.shape[0] not in [3, 4]:\n",
    "        raise Exception(\"Input points should be (3,N) or (4,N) in shape, found {}\".format(points.shape))\n",
    "\n",
    "    tm = create_transformation_matrix_to_voxel_space(shape, voxel_size, (0, 0, z_offset))\n",
    "    p = transform_points(points, tm) #(3, N)\n",
    "    return p\n",
    "\n",
    "def create_voxel_pointcloud(points, shape, voxel_size=(0.5,0.5,1), z_offset=0):\n",
    "\n",
    "    points_voxel_coords = car_to_voxel_coords(points.copy(), shape, voxel_size, z_offset) #(3,N)\n",
    "    points_voxel_coords = points_voxel_coords[:3].transpose(1,0) #(N, 3)\n",
    "    points_voxel_coords = np.int0(points_voxel_coords)\n",
    "    bev = np.zeros(shape, dtype=np.float32)\n",
    "    bev_shape = np.array(shape)\n",
    "    within_bounds = (np.all(points_voxel_coords >= 0, axis=1) * np.all(\n",
    "        points_voxel_coords < bev_shape, axis=1))\n",
    "    points_voxel_coords = points_voxel_coords[within_bounds]\n",
    "    coord, count = np.unique(points_voxel_coords, axis=0, return_counts=True)\n",
    "    # Note X and Y are flipped:\n",
    "    bev[coord[:,1], coord[:,0], coord[:,2]] = count\n",
    "    \n",
    "    return bev\n",
    "\n",
    "def normalize_voxel_intensities(bev, max_intensity=16):\n",
    "    return (bev/max_intensity).clip(0,1)\n",
    "\n",
    "def move_boxes_to_car_space(boxes, ego_pose):\n",
    "    \"\"\"\n",
    "    Move boxes from world space to car space.\n",
    "    Note: mutates input boxes.\n",
    "    \"\"\"\n",
    "    translation = -np.array(ego_pose['translation'])\n",
    "    rotation = Quaternion(ego_pose['rotation']).inverse\n",
    "    for box in boxes:\n",
    "        # Bring box to car space\n",
    "        box.translate(translation)\n",
    "        box.rotate(rotation)\n",
    "        \n",
    "def scale_boxes(boxes, factor):\n",
    "    \"\"\"\n",
    "    Note: mutates input boxes\n",
    "    \"\"\"\n",
    "    for box in boxes:\n",
    "        box.wlh = box.wlh * factor\n",
    "\n",
    "def draw_boxes(im, voxel_size, boxes, classes, z_offset=0.0):\n",
    "    for box in boxes:\n",
    "        # We only care about the bottom corners\n",
    "        corners = box.bottom_corners()\n",
    "        corners_voxel = car_to_voxel_coords(corners, im.shape, voxel_size, z_offset).transpose(1,0)\n",
    "        corners_voxel = corners_voxel[:,:2] # Drop z coord\n",
    "        class_color = classes.index(box.name) + 1\n",
    "        if class_color == 0:\n",
    "            raise Exception(\"Unknown class: {}\".format(box.name))\n",
    "        cv2.drawContours(im, np.int0([corners_voxel]), 0, (class_color, class_color, class_color), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEVImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_filepaths, target_filepaths, map_filepaths=None):\n",
    "        self.input_filepaths = input_filepaths\n",
    "        self.target_filepaths = target_filepaths\n",
    "        self.map_filepaths = map_filepaths\n",
    "        \n",
    "        if map_filepaths is not None:\n",
    "            assert len(input_filepaths) == len(map_filepaths)        \n",
    "        assert len(input_filepaths) == len(target_filepaths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_filepath = self.input_filepaths[idx]\n",
    "        target_filepath = self.target_filepaths[idx]        \n",
    "        sample_token = input_filepath.split(\"/\")[-1].replace(\"_input.png\",\"\")        \n",
    "        im = cv2.imread(input_filepath, cv2.IMREAD_UNCHANGED)        \n",
    "        if self.map_filepaths:\n",
    "            map_filepath = self.map_filepaths[idx]\n",
    "            map_im = cv2.imread(map_filepath, cv2.IMREAD_UNCHANGED)\n",
    "            im = np.concatenate((im, map_im), axis=2)        \n",
    "        target = cv2.imread(target_filepath, cv2.IMREAD_UNCHANGED)        \n",
    "        im = im.astype(np.float32)/255\n",
    "        target = target.astype(np.int64)        \n",
    "        im = torch.from_numpy(im.transpose(2,0,1))\n",
    "        target = torch.from_numpy(target)      \n",
    "        return im, target, sample_token\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=2,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=False,\n",
    "        batch_norm=False,\n",
    "        up_mode='upconv',\n",
    "    ):\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        \n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1) # dropout is absent\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i == len(self.down_path) - 1:\n",
    "                x = self.dropout(x)\n",
    "                continue\n",
    "            elif i == len(self.down_path) - 2:\n",
    "                x = self.dropout(x)\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "            else:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "18421 instance,\n",
      "10 sensor,\n",
      "148 calibrated_sensor,\n",
      "177789 ego_pose,\n",
      "180 log,\n",
      "180 scene,\n",
      "22680 sample,\n",
      "189504 sample_data,\n",
      "638179 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 7.5 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 3.3 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "level5data = LyftDataset(data_path='.', json_path='/home/bob/data/lyft_data/train_data', verbose=True)\n",
    "records = [(level5data.get('sample', record['first_sample_token'])['timestamp'], record) for record in\n",
    "        level5data.scene]\n",
    "entries = []\n",
    "for start_time, record in sorted(records):\n",
    "    start_time = level5data.get('sample', record['first_sample_token'])['timestamp'] / 1000000\n",
    "\n",
    "    token = record['token']\n",
    "    name = record['name']\n",
    "    date = datetime.utcfromtimestamp(start_time)\n",
    "    host = \"-\".join(record['name'].split(\"-\")[:2])\n",
    "    first_sample_token = record[\"first_sample_token\"]\n",
    "    entries.append((host, name, date, token, first_sample_token))\n",
    "df = pd.DataFrame(entries, columns=[\"host\", \"scene_name\", \"date\", \"scene_token\", \"first_sample_token\"])\n",
    "\n",
    "validation_hosts = [\"host-a007\", \"host-a008\", \"host-a009\"]\n",
    "validation_df = df[df[\"host\"].isin(validation_hosts)]\n",
    "vi = validation_df.index\n",
    "train_df = df[~df.index.isin(vi)]\n",
    "classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \n",
    "           \"other_vehicle\", \"animal\", \"emergency_vehicle\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Some hyperparameters we'll need to define for the system\n",
    "voxel_size = (0.3, 0.3, 1.0)\n",
    "z_offset = -2.0\n",
    "bev_shape = (496, 496, 3)\n",
    "# We scale down each box so they are more separated when projected into our coarse voxel space.\n",
    "box_scale = 0.9\n",
    "\n",
    "batch_size = 2\n",
    "epochs = 15\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_weights = torch.from_numpy(np.array([0.2] + [1.0]*len(classes), dtype=np.float32))\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_folder = os.path.join(ARTIFACTS_FOLDER, \"bev_train_data\")\n",
    "input_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_input.png\")))\n",
    "target_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_target.png\")))\n",
    "#map_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_map.png\")))\n",
    "train_dataset = BEVImageDataset(input_filepaths, target_filepaths)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, num_workers=os.cpu_count())\n",
    "\n",
    "validation_data_folder = os.path.join(ARTIFACTS_FOLDER, \"./bev_validation_data\")\n",
    "val_input_filepaths = sorted(glob.glob(os.path.join(validation_data_folder, \"*_input.png\")))\n",
    "val_target_filepaths = sorted(glob.glob(os.path.join(validation_data_folder, \"*_target.png\")))\n",
    "#map_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_map.png\")))\n",
    "val_dataset = BEVImageDataset(val_input_filepaths, val_target_filepaths)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=True, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet_model(in_channels=6, num_output_classes=2):\n",
    "    model = UNet(in_channels=in_channels, n_classes=num_output_classes, wf=6, depth=5, padding=True, \n",
    "                 up_mode='upsample')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b26c240ce34d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#model = get_unet_model(in_channels=3, num_output_classes=len(classes)+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDepthCompletionNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_out_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#pretrained_filename = \"unet/unet_epoch_15.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d586081114b8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_out_channels, res_layers, pretrained)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_bn_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mpretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'resnet{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resnet' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "#model = get_unet_model(in_channels=3, num_output_classes=len(classes)+1)\n",
    "model = DepthCompletionNet(num_out_channels=len(classes)+1)\n",
    "model = model.to(device)\n",
    "#pretrained_filename = \"unet/unet_epoch_15.pth\"\n",
    "#pretrained_filepath = os.path.join(ARTIFACTS_FOLDER, pretrained_filename)\n",
    "#model.load_state_dict(torch.load(pretrained_filepath))\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, 0.7) #optimizer, gamma, last_epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "os.makedirs('./artifacts/resnet34_496', exist_ok=True)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    ## Train\n",
    "    print(\"Epoch\", epoch)\n",
    "    train_epoch_losses = []\n",
    "    train_bar = tqdm_notebook(trainloader)\n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "    for ii, (X, target, sample_ids) in enumerate(train_bar):\n",
    "        X = X.to(device)  # [N, 3, H, W]\n",
    "        target = target.to(device)  # [N, H, W] with class indices (0, 1)\n",
    "        prediction = model(X)  # [N, 2, H, W]\n",
    "        loss = F.cross_entropy(prediction, target, weight=class_weights)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()        \n",
    "        train_epoch_losses.append(loss.detach().cpu().numpy())\n",
    "        #if ii == 0:            \n",
    "         #   visualize_predictions(X, prediction, target)    \n",
    "    train_loss = np.mean(train_epoch_losses)\n",
    "    train_losses.append(train_loss)\n",
    "    print(\"Train Loss:\", train_loss)\n",
    "    checkpoint_filename = \"resnet34_496/resnet_epoch_{}.pth\".format(epoch)\n",
    "    checkpoint_filepath = os.path.join(ARTIFACTS_FOLDER, checkpoint_filename)\n",
    "    torch.save(model.state_dict(), checkpoint_filepath)\n",
    "    print(checkpoint_filepath)\n",
    "    \n",
    "    ## Val\n",
    "    val_epoch_losses = []\n",
    "    val_bar = tqdm_notebook(valloader)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for ii, (val_X, val_target, batch_sample_tokens) in enumerate(val_bar):\n",
    "            val_X = val_X.to(device)  # [N, 1, H, W]\n",
    "            val_target = val_target.to(device)  # [N, H, W] with class indices (0, 1)\n",
    "            prediction = model(X)  # [N, 2, H, W]\n",
    "            loss = F.cross_entropy(prediction, target, weight=class_weights)\n",
    "            val_epoch_losses.append(loss.detach().cpu().numpy())\n",
    "    val_loss = np.mean(val_epoch_losses)\n",
    "    val_losses.append(val_loss)\n",
    "    print(\"Val loss:\", np.mean(val_loss))\n",
    "x = range(1, epochs+1)\n",
    "plt.plot(x, train_losses, 'bo', linestyle='dashed', linewidth=2, markersize=12)\n",
    "plt.plot(x, val_losses, 'g*', linestyle='dashed', linewidth=2, markersize=12)\n",
    "plt.legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mAP on Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "from lyft_dataset_sdk.eval.detection.mAP_evaluation import Box3D, recall_precision\n",
    "\n",
    "def val_map(epoch):\n",
    "    ## load model\n",
    "    model = get_unet_model(in_channels=3, num_output_classes=1+len(classes))\n",
    "    model = model.to(device)\n",
    "    checkpoint_filename = \"resnet34_496/resnet_epoch_{}.pth\".format(epoch)\n",
    "    checkpoint_filepath = os.path.join(ARTIFACTS_FOLDER, checkpoint_filename)\n",
    "    model.load_state_dict(torch.load(checkpoint_filepath))\n",
    "    \n",
    "    gc.collect()\n",
    "    progress_bar = tqdm_notebook(valloader)\n",
    "    targets = np.zeros((len(val_target_filepaths), bev_shape[0], bev_shape[1]), dtype=np.uint8)\n",
    "    # We quantize to uint8 here to conserve memory. We're allocating >20GB of memory otherwise.\n",
    "    predictions = np.zeros((len(val_target_filepaths), 1+len(classes), bev_shape[0], \n",
    "                            bev_shape[1]), dtype=np.uint8)\n",
    "    sample_tokens = []\n",
    "    \n",
    "    ## predict\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for ii, (X, target, batch_sample_tokens) in enumerate(progress_bar):\n",
    "            offset = ii*batch_size\n",
    "            targets[offset:offset+batch_size] = target.numpy()\n",
    "            sample_tokens.extend(batch_sample_tokens)\n",
    "            X = X.to(device)  # [N, 1, H, W]\n",
    "            target = target.to(device)  # [N, H, W] with class indices (0, 1)\n",
    "            prediction = model(X)  # [N, 2, H, W]\n",
    "            prediction = F.softmax(prediction, dim=1)\n",
    "            prediction_cpu = prediction.cpu().numpy()\n",
    "            predictions[offset:offset+batch_size] = np.round(prediction_cpu*255).astype(np.uint8)\n",
    "    predictions_non_class0 = 255 - predictions[:,0]   \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
    "    predictions_opened = np.zeros((predictions_non_class0.shape), dtype=np.uint8)\n",
    "    background_threshold = 255//2\n",
    "    for i, p in enumerate(tqdm(predictions_non_class0)):\n",
    "        thresholded_p = (p > background_threshold).astype(np.uint8)\n",
    "        predictions_opened[i] = cv2.morphologyEx(thresholded_p, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    ## 2d detection\n",
    "    detection_boxes = []\n",
    "    detection_scores = []\n",
    "    detection_classes = []\n",
    "\n",
    "    for i in tqdm_notebook(range(len(predictions))):\n",
    "        prediction_opened = predictions_opened[i]\n",
    "        probability_non_class0 = predictions_non_class0[i]\n",
    "        class_probability = predictions[i]\n",
    "        sample_boxes = []\n",
    "        sample_detection_scores = []\n",
    "        sample_detection_classes = []\n",
    "        contours, hierarchy = cv2.findContours(prediction_opened, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)     \n",
    "        for cnt in contours:\n",
    "            rect = cv2.minAreaRect(cnt)\n",
    "            box = cv2.boxPoints(rect)        \n",
    "            # Let's take the center pixel value as the confidence value\n",
    "            box_center_index = np.int0(np.mean(box, axis=0))        \n",
    "            for class_index in range(len(classes)):\n",
    "                box_center_value = class_probability[class_index+1, box_center_index[1], box_center_index[0]]            \n",
    "                # Let's remove candidates with very low probability\n",
    "                if box_center_value < 0.01:\n",
    "                    continue            \n",
    "                box_center_class = classes[class_index]\n",
    "                box_detection_score = box_center_value\n",
    "                sample_detection_classes.append(box_center_class)\n",
    "                sample_detection_scores.append(box_detection_score)\n",
    "                sample_boxes.append(box)            \n",
    "        detection_boxes.append(np.array(sample_boxes))\n",
    "        detection_scores.append(sample_detection_scores)\n",
    "        detection_classes.append(sample_detection_classes)\n",
    "    \n",
    "    ## 3d detection\n",
    "    pred_box3ds = []\n",
    "    for (sample_token, sample_boxes, sample_detection_scores, sample_detection_class) in tqdm_notebook(\n",
    "            zip(sample_tokens, detection_boxes, detection_scores, \n",
    "            detection_classes), total=len(sample_tokens)):\n",
    "        sample_boxes = sample_boxes.reshape(-1, 2) # (N, 4, 2) -> (N*4, 2)\n",
    "        sample_boxes = sample_boxes.transpose(1,0) # (N*4, 2) -> (2, N*4)\n",
    "        # Add Z dimension\n",
    "        sample_boxes = np.vstack((sample_boxes, np.zeros(sample_boxes.shape[1]),)) # (2, N*4) -> (3, N*4)\n",
    "        sample = level5data.get(\"sample\", sample_token)\n",
    "        sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "        lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "        lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "        ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "        ego_translation = np.array(ego_pose['translation'])\n",
    "        global_from_car = transform_matrix(ego_pose['translation'],\n",
    "                                       Quaternion(ego_pose['rotation']), inverse=False)\n",
    "        car_from_voxel = np.linalg.inv(create_transformation_matrix_to_voxel_space(\n",
    "                                        bev_shape, voxel_size, (0, 0, z_offset)))\n",
    "        global_from_voxel = np.dot(global_from_car, car_from_voxel)\n",
    "        sample_boxes = transform_points(sample_boxes, global_from_voxel)\n",
    "        sample_boxes[2,:] = ego_pose[\"translation\"][2]\n",
    "        # (3, N*4) -> (N, 4, 3)\n",
    "        sample_boxes = sample_boxes.transpose(1,0).reshape(-1, 4, 3)\n",
    "        # We don't know the height of our boxes, let's assume every object is the same height.\n",
    "        box_height = 1.75\n",
    "        sample_boxes_centers = sample_boxes.mean(axis=1)\n",
    "        sample_boxes_centers[:,2] += box_height/2    \n",
    "        sample_lengths = np.linalg.norm(sample_boxes[:,0,:] - sample_boxes[:,1,:], axis=1) * 1/box_scale\n",
    "        sample_widths = np.linalg.norm(sample_boxes[:,1,:] - sample_boxes[:,2,:], axis=1) * 1/box_scale    \n",
    "        sample_boxes_dimensions = np.zeros_like(sample_boxes_centers) \n",
    "        sample_boxes_dimensions[:,0] = sample_widths\n",
    "        sample_boxes_dimensions[:,1] = sample_lengths\n",
    "        sample_boxes_dimensions[:,2] = box_height\n",
    "        for i in range(len(sample_boxes)):\n",
    "            translation = sample_boxes_centers[i]\n",
    "            size = sample_boxes_dimensions[i]\n",
    "            class_name = sample_detection_class[i]\n",
    "            ego_distance = float(np.linalg.norm(ego_translation - translation))          \n",
    "            # Determine the rotation of the box\n",
    "            v = (sample_boxes[i,0] - sample_boxes[i,1])\n",
    "            v /= np.linalg.norm(v)\n",
    "            r = R.from_dcm([ # initialization from direction cosine matrices\n",
    "                [v[0], -v[1], 0],\n",
    "                [v[1],  v[0], 0],\n",
    "                [   0,     0, 1],\n",
    "            ])\n",
    "            quat = r.as_quat() # as quaternions\n",
    "            # XYZW -> WXYZ order of elements\n",
    "            quat = quat[[3,0,1,2]]        \n",
    "            detection_score = float(sample_detection_scores[i])        \n",
    "            box3d = Box3D(\n",
    "                sample_token=sample_token,\n",
    "                translation=list(translation),\n",
    "                size=list(size),\n",
    "                rotation=list(quat),\n",
    "                name=class_name,\n",
    "                score=detection_score\n",
    "            )\n",
    "            pred_box3ds.append(box3d)\n",
    "    \n",
    "    ## mAP calc\n",
    "    pred = [b.serialize() for b in pred_box3ds]\n",
    "    with open(os.path.join(ARTIFACTS_FOLDER, \"json/unet500/pred_{}.json\".format(epoch)), \"w\") as f:\n",
    "        json.dump(pred, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc7bcfb993842a49af138259bfffe85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=630), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/anaconda3/envs/torch37/lib/python3.7/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5040/5040 [00:01<00:00, 4235.28it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba5e6ebeb9743baa397537c1640de6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5040), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cef1524241490aa95b10d5658afb7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5040), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd2285b61b74d3c96b8671f308fc784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=630), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5040/5040 [00:01<00:00, 4025.14it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a6429a899e4582b1078dbe43236883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5040), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c73afeb7a24ab3985ecc2dcd389f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5040), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_map(epoch=12)\n",
    "val_map(epoch=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.1050620193022741\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.03192542878666849)\n",
      "('bus', 0.051367276695549226)\n",
      "('car', 0.485934698561018)\n",
      "('motorcycle', 0.012984378170014202)\n",
      "('other_vehicle', 0.24332543008689356)\n",
      "('pedestrian', 0.007177650093509974)\n",
      "('truck', 0.007781292024539437)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/unet500/pred_11.json' --gt/home/bob/code/lyft/eval_file \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.10303964494351976\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.020240364505013236)\n",
      "('bus', 0.05615950939036278)\n",
      "('car', 0.4896918134879935)\n",
      "('motorcycle', 0.002967682212965232)\n",
      "('other_vehicle', 0.24063487119444624)\n",
      "('pedestrian', 0.006619156133551654)\n",
      "('truck', 0.008003762623825527)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/unet500/pred_10.json' --gt_file \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.09696393948890356\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.020522331424150303)\n",
      "('bus', 0.044108999504336756)\n",
      "('car', 0.4760304025770559)\n",
      "('motorcycle', 0.00936083077373117)\n",
      "('other_vehicle', 0.21244530272431872)\n",
      "('pedestrian', 0.006776530621727134)\n",
      "('truck', 0.0064671182859085494)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/unet500/pred_12.json' --gt_file \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.106139026940494\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.03194584508237589)\n",
      "('bus', 0.05485744524089356)\n",
      "('car', 0.4819533836688439)\n",
      "('motorcycle', 0.009448588562234898)\n",
      "('other_vehicle', 0.2553146374039543)\n",
      "('pedestrian', 0.0084616114672506)\n",
      "('truck', 0.0071307040983989)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/unet500/pred_13.json' --gt_file \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.09914757147483642\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.011524515325533953)\n",
      "('bus', 0.06473120511491712)\n",
      "('car', 0.47667239016310103)\n",
      "('motorcycle', 0.004528301886792452)\n",
      "('other_vehicle', 0.2261457226031809)\n",
      "('pedestrian', 0.0029124416545519916)\n",
      "('truck', 0.006665995050613903)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/unet/pred_7.json' --gt_file \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.10267539615864357\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.0049842166323256325)\n",
      "('bus', 0.06498048608258282)\n",
      "('car', 0.4997703019744049)\n",
      "('motorcycle', 0.005302449250907603)\n",
      "('other_vehicle', 0.23059450862261271)\n",
      "('pedestrian', 0.0034707821531930003)\n",
      "('truck', 0.012300424553121834)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/unet/pred_8.json' --gt_file \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.10058817970429339\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.00501539544602367)\n",
      "('bus', 0.0661227842668923)\n",
      "('car', 0.4812704256633357)\n",
      "('motorcycle', 0.0014483127617571632)\n",
      "('other_vehicle', 0.23472350780008439)\n",
      "('pedestrian', 0.004214098687682446)\n",
      "('truck', 0.011910913008571404)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/unet/pred_9.json' --gt_file \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.09661872671003473\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.006207686512882227)\n",
      "('bus', 0.05365747843992068)\n",
      "('car', 0.4625923395543592)\n",
      "('motorcycle', 0.0013331993995301156)\n",
      "('other_vehicle', 0.23447758587535306)\n",
      "('pedestrian', 0.002867096202367134)\n",
      "('truck', 0.011814427695865532)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/unet/pred_10.json' --gt_file \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.1083809834325789\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.014377314889907412)\n",
      "('bus', 0.07868036752077714)\n",
      "('car', 0.4879433012496469)\n",
      "('motorcycle', 0.0036513035763962283)\n",
      "('other_vehicle', 0.2664949674229056)\n",
      "('pedestrian', 0.0045337745158624595)\n",
      "('truck', 0.011366838285135477)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/unet/pred_11.json' --gt_file \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.1011208595355569\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.018045573384186407)\n",
      "('bus', 0.06715809217886828)\n",
      "('car', 0.4721368518106879)\n",
      "('motorcycle', 0.0037810721772985924)\n",
      "('other_vehicle', 0.23084642142279588)\n",
      "('pedestrian', 0.0045671306990924)\n",
      "('truck', 0.012431734611525764)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/unet/pred_12.json' --gt_file \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.1073798958101321\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.009746183221966291)\n",
      "('bus', 0.0753229748620661)\n",
      "('car', 0.4861778906690318)\n",
      "('motorcycle', 0.0016264120037704944)\n",
      "('other_vehicle', 0.2695621634185528)\n",
      "('pedestrian', 0.004223498539495018)\n",
      "('truck', 0.012380043766174222)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/unet/pred_13.json' --gt_file \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.10645109231341443\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.01385349517747148)\n",
      "('bus', 0.08394761171372919)\n",
      "('car', 0.463558322179128)\n",
      "('motorcycle', 0.02210876986253622)\n",
      "('other_vehicle', 0.25726352624599613)\n",
      "('pedestrian', 0.003355396881664816)\n",
      "('truck', 0.0075216164467896635)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/unet/pred_14.json' --gt_file \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.1078577617762986\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.013198872017253593)\n",
      "('bus', 0.07745053242650779)\n",
      "('car', 0.4916629343578503)\n",
      "('motorcycle', 0.002830188679245283)\n",
      "('other_vehicle', 0.2622120155137613)\n",
      "('pedestrian', 0.0032572098740683572)\n",
      "('truck', 0.012250341341702343)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/unet/pred_15.json' --gt_file \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
