{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这部分是模型训练与验证的主要代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS_FOLDER = \"./artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Disable multiprocesing for numpy/opencv. We already multiprocess ourselves, this would mean every subprocess produces\n",
    "# even more threads which would lead to a lot of context switching, slowing things down a lot.\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformation_matrix_to_voxel_space(shape, voxel_size, offset):\n",
    "    \"\"\"\n",
    "    Constructs a transformation matrix given an output voxel shape such that (0,0,0) ends up in the center.\n",
    "    Voxel_size defines how large every voxel is in world coordinate, \n",
    "    (1,1,1) would be the same as Minecraft voxels.\n",
    "    An offset per axis in world coordinates (metric) can be provided, \n",
    "    this is useful for Z (up-down) in lidar points.\n",
    "    No rotation, only a scale and translation.\n",
    "    \"\"\"\n",
    "    \n",
    "    shape, voxel_size, offset = np.array(shape), np.array(voxel_size), np.array(offset)\n",
    "    tm = np.eye(4, dtype=np.float32) #(4,4)\n",
    "    translation = shape/2 + offset/voxel_size #(3,)\n",
    "    tm = tm * np.array(np.hstack((1/voxel_size, [1]))) # (4,4)\n",
    "    tm[:3, 3] = np.transpose(translation)\n",
    "    return tm\n",
    "\n",
    "def transform_points(points, transf_matrix):\n",
    "    \"\"\"\n",
    "    Transform (3,N) or (4,N) points using transformation matrix.\n",
    "    \"\"\"\n",
    "    if points.shape[0] not in [3,4]:\n",
    "        raise Exception(\"Points input should be (3,N) or (4,N) shape, received {}\".format(points.shape))\n",
    "    return transf_matrix.dot(np.vstack((points[:3, :], np.ones(points.shape[1]))))[:3, :]\n",
    "\n",
    "def car_to_voxel_coords(points, shape, voxel_size, z_offset=0):\n",
    "    if len(shape) != 3:\n",
    "        raise Exception(\"Voxel volume shape should be 3 dimensions (x,y,z)\")\n",
    "        \n",
    "    if len(points.shape) != 2 or points.shape[0] not in [3, 4]:\n",
    "        raise Exception(\"Input points should be (3,N) or (4,N) in shape, found {}\".format(points.shape))\n",
    "\n",
    "    tm = create_transformation_matrix_to_voxel_space(shape, voxel_size, (0, 0, z_offset))\n",
    "    p = transform_points(points, tm) #(3, N)\n",
    "    return p\n",
    "\n",
    "def create_voxel_pointcloud(points, shape, voxel_size=(0.5,0.5,1), z_offset=0):\n",
    "\n",
    "    points_voxel_coords = car_to_voxel_coords(points.copy(), shape, voxel_size, z_offset) #(3,N)\n",
    "    points_voxel_coords = points_voxel_coords[:3].transpose(1,0) #(N, 3)\n",
    "    points_voxel_coords = np.int0(points_voxel_coords)\n",
    "    bev = np.zeros(shape, dtype=np.float32)\n",
    "    bev_shape = np.array(shape)\n",
    "    within_bounds = (np.all(points_voxel_coords >= 0, axis=1) * np.all(\n",
    "        points_voxel_coords < bev_shape, axis=1))\n",
    "    points_voxel_coords = points_voxel_coords[within_bounds]\n",
    "    coord, count = np.unique(points_voxel_coords, axis=0, return_counts=True)\n",
    "    # Note X and Y are flipped:\n",
    "    bev[coord[:,1], coord[:,0], coord[:,2]] = count\n",
    "    \n",
    "    return bev\n",
    "\n",
    "def normalize_voxel_intensities(bev, max_intensity=16):\n",
    "    return (bev/max_intensity).clip(0,1)\n",
    "\n",
    "def move_boxes_to_car_space(boxes, ego_pose):\n",
    "    \"\"\"\n",
    "    Move boxes from world space to car space.\n",
    "    Note: mutates input boxes.\n",
    "    \"\"\"\n",
    "    translation = -np.array(ego_pose['translation'])\n",
    "    rotation = Quaternion(ego_pose['rotation']).inverse\n",
    "    for box in boxes:\n",
    "        # Bring box to car space\n",
    "        box.translate(translation)\n",
    "        box.rotate(rotation)\n",
    "        \n",
    "def scale_boxes(boxes, factor):\n",
    "    \"\"\"\n",
    "    Note: mutates input boxes\n",
    "    \"\"\"\n",
    "    for box in boxes:\n",
    "        box.wlh = box.wlh * factor\n",
    "\n",
    "def draw_boxes(im, voxel_size, boxes, classes, z_offset=0.0):\n",
    "    for box in boxes:\n",
    "        # We only care about the bottom corners\n",
    "        corners = box.bottom_corners()\n",
    "        corners_voxel = car_to_voxel_coords(corners, im.shape, voxel_size, z_offset).transpose(1,0)\n",
    "        corners_voxel = corners_voxel[:,:2] # Drop z coord\n",
    "        class_color = classes.index(box.name) + 1\n",
    "        if class_color == 0:\n",
    "            raise Exception(\"Unknown class: {}\".format(box.name))\n",
    "        cv2.drawContours(im, np.int0([corners_voxel]), 0, (class_color, class_color, class_color), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEVImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_filepaths, target_filepaths, map_filepaths=None):\n",
    "        self.input_filepaths = input_filepaths\n",
    "        self.target_filepaths = target_filepaths\n",
    "        self.map_filepaths = map_filepaths\n",
    "        \n",
    "        if map_filepaths is not None:\n",
    "            assert len(input_filepaths) == len(map_filepaths)        \n",
    "        assert len(input_filepaths) == len(target_filepaths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_filepath = self.input_filepaths[idx]\n",
    "        target_filepath = self.target_filepaths[idx]        \n",
    "        sample_token = input_filepath.split(\"/\")[-1].replace(\"_input.png\",\"\")        \n",
    "        im = cv2.imread(input_filepath, cv2.IMREAD_UNCHANGED)        \n",
    "        if self.map_filepaths:\n",
    "            map_filepath = self.map_filepaths[idx]\n",
    "            map_im = cv2.imread(map_filepath, cv2.IMREAD_UNCHANGED)\n",
    "            im = np.concatenate((im, map_im), axis=2)        \n",
    "        target = cv2.imread(target_filepath, cv2.IMREAD_UNCHANGED)        \n",
    "        im = im.astype(np.float32)/255\n",
    "        target = target.astype(np.int64)        \n",
    "        im = torch.from_numpy(im.transpose(2,0,1))\n",
    "        target = torch.from_numpy(target)      \n",
    "        return im, target, sample_token\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=2,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=False,\n",
    "        batch_norm=False,\n",
    "        up_mode='upconv',\n",
    "    ):\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        \n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1) # dropout is absent\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i == len(self.down_path) - 1:\n",
    "                x = self.dropout(x)\n",
    "                continue\n",
    "            elif i == len(self.down_path) - 2:\n",
    "                x = self.dropout(x)\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "            else:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ResUnet Model\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(0, 1e-3)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.ConvTranspose2d):\n",
    "        m.weight.data.normal_(0, 1e-3)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "def conv_bn_relu(in_channels, out_channels, kernel_size, \\\n",
    "        stride=1, padding=0, bn=True, relu=True):\n",
    "    bias = not bn\n",
    "    layers = []\n",
    "    layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride,\n",
    "        padding, bias=bias))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    if relu:\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    layers = nn.Sequential(*layers)\n",
    "\n",
    "    # initialize the weights\n",
    "    for m in layers.modules():\n",
    "        init_weights(m)\n",
    "\n",
    "    return layers\n",
    "\n",
    "def convt_bn_relu(in_channels, out_channels, kernel_size, \\\n",
    "        stride=1, padding=0, output_padding=0, bn=True, relu=True):\n",
    "    bias = not bn\n",
    "    layers = []\n",
    "    layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\n",
    "        stride, padding, output_padding, bias=bias))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    if relu:\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    layers = nn.Sequential(*layers)\n",
    "\n",
    "    # initialize the weights\n",
    "    for m in layers.modules():\n",
    "        init_weights(m)\n",
    "\n",
    "    return layers\n",
    "\n",
    "class DepthCompletionNet(nn.Module):\n",
    "    def __init__(self, num_out_channels, res_layers=34, pretrained=False):\n",
    "        assert (res_layers in [18, 34, 50, 101, 152]\n",
    "               ),'Only layers 18, 34, 50, 101, and 152 are defined, but got {}'.format(res_layers)\n",
    "        super(DepthCompletionNet, self).__init__()\n",
    "        \n",
    "        channels = 64\n",
    "        self.conv1_img = conv_bn_relu(3, channels, kernel_size=3, stride=1, padding=1)\n",
    "        pretrained_model = resnet.__dict__['resnet{}'.format(res_layers)](pretrained=pretrained)\n",
    "        if not pretrained:\n",
    "            pretrained_model.apply(init_weights)\n",
    "        #self.maxpool = pretrained_model._modules['maxpool']\n",
    "        self.conv2 = pretrained_model._modules['layer1']\n",
    "        self.conv3 = pretrained_model._modules['layer2']\n",
    "        self.conv4 = pretrained_model._modules['layer3']\n",
    "        self.conv5 = pretrained_model._modules['layer4']\n",
    "        del pretrained_model # clear memory\n",
    "\n",
    "        # define number of intermediate channels\n",
    "        if res_layers <= 34:\n",
    "            num_channels = 512\n",
    "        elif res_layers >= 50:\n",
    "            num_channels = 2048\n",
    "        self.conv6 = conv_bn_relu(num_channels, 512, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # decoding layers\n",
    "        kernel_size = 3\n",
    "        stride = 2\n",
    "        self.convt5 = convt_bn_relu(in_channels=512, out_channels=256,\n",
    "            kernel_size=kernel_size, stride=stride, padding=1, output_padding=1)\n",
    "        self.convt4 = convt_bn_relu(in_channels=768, out_channels=128,\n",
    "            kernel_size=kernel_size, stride=stride, padding=1, output_padding=1)\n",
    "        self.convt3 = convt_bn_relu(in_channels=(256+128), out_channels=64,\n",
    "            kernel_size=kernel_size, stride=stride, padding=1, output_padding=1)\n",
    "        self.convt2 = convt_bn_relu(in_channels=(128+64), out_channels=64,\n",
    "            kernel_size=kernel_size, stride=stride, padding=1, output_padding=1)\n",
    "        self.convt1 = convt_bn_relu(in_channels=128, out_channels=64,\n",
    "            kernel_size=kernel_size, stride=1, padding=1)\n",
    "        self.convtf = conv_bn_relu(in_channels=128, out_channels=num_out_channels, kernel_size=1, \n",
    "                                   stride=1, bn=False, relu=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # first layer\n",
    "        conv1 = self.conv1_img(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2) # batchsize * ? * 176 * 608\n",
    "        conv4 = self.conv4(conv3) # batchsize * ? * 88 * 304\n",
    "        conv5 = self.conv5(conv4) # batchsize * ? * 44 * 152\n",
    "        conv6 = self.conv6(conv5) # batchsize * ? * 22 * 76\n",
    "\n",
    "        # decoder\n",
    "        convt5 = self.convt5(conv6)\n",
    "        y = torch.cat((convt5, conv5), 1)\n",
    "\n",
    "        convt4 = self.convt4(y)\n",
    "        y = torch.cat((convt4, conv4), 1)\n",
    "\n",
    "        convt3 = self.convt3(y)\n",
    "        y = torch.cat((convt3, conv3), 1)\n",
    "\n",
    "        convt2 = self.convt2(y)\n",
    "        y = torch.cat((convt2, conv2), 1)\n",
    "\n",
    "        convt1 = self.convt1(y)\n",
    "        y = torch.cat((convt1,conv1), 1)\n",
    "\n",
    "        y = self.convtf(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \n",
    "           \"other_vehicle\", \"animal\", \"emergency_vehicle\"]\n",
    "#train_data_folder = os.path.join(ARTIFACTS_FOLDER, \"bev_train_data\")\n",
    "#input_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_input.png\")))\n",
    "#target_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_target.png\")))\n",
    "#map_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_map.png\")))\n",
    "#train_dataset = BEVImageDataset(input_filepaths, target_filepaths)\n",
    "#trainloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, num_workers=os.cpu_count())\n",
    "batch_size = 8\n",
    "validation_data_folder = os.path.join(ARTIFACTS_FOLDER, \"./bev_validation_data\")\n",
    "val_input_filepaths = sorted(glob.glob(os.path.join(validation_data_folder, \"*_input.png\")))\n",
    "val_target_filepaths = sorted(glob.glob(os.path.join(validation_data_folder, \"*_target.png\")))\n",
    "#map_filepaths = sorted(glob.glob(os.path.join(train_data_folder, \"*_map.png\")))\n",
    "val_dataset = BEVImageDataset(val_input_filepaths, val_target_filepaths)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=True, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Some hyperparameters we'll need to define for the system\n",
    "voxel_size = (0.3, 0.3, 1.0)\n",
    "z_offset = -2.0\n",
    "bev_shape = (496, 496, 3)\n",
    "# We scale down each box so they are more separated when projected into our coarse voxel space.\n",
    "box_scale = 0.9\n",
    "epochs = 15\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_weights = torch.from_numpy(np.array([0.2] + [1.0]*len(classes), dtype=np.float32))\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet_model(in_channels=6, num_output_classes=2):\n",
    "    model = UNet(in_channels=in_channels, n_classes=num_output_classes, wf=6, depth=5, padding=True, \n",
    "                 up_mode='upsample')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mAP on Val\n",
    "### 计算模型在验证集上的mAP\n",
    "1. 确定预测目标框\n",
    "2. 利用mAP_evaluation.py计算mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "15991 instance,\n",
      "8 sensor,\n",
      "128 calibrated_sensor,\n",
      "149072 ego_pose,\n",
      "148 log,\n",
      "148 scene,\n",
      "18634 sample,\n",
      "149072 sample_data,\n",
      "539765 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 6.4 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 2.5 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "## 读入初始点云数据,后面对语义分割预测结果进行后处理时,需要用到车身的角度等信息\n",
    "level5data = LyftDataset(data_path='.', json_path='/home/bob/data/lyft_data/train_data', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "from lyft_dataset_sdk.eval.detection.mAP_evaluation import Box3D, recall_precision\n",
    "from torchvision.models import resnet\n",
    "\n",
    "def val_map(reslayers, resolution, epoch):\n",
    "    ## load model\n",
    "    model = DepthCompletionNet(num_out_channels=len(classes)+1, res_layers=reslayers)\n",
    "    model = model.to(device)\n",
    "    checkpoint_filename = \"resnet{}_{}/resnet_epoch_{}.pth\".format(reslayers, resolution, epoch)\n",
    "    checkpoint_filepath = os.path.join(ARTIFACTS_FOLDER, checkpoint_filename)\n",
    "    model.load_state_dict(torch.load(checkpoint_filepath))\n",
    "    \n",
    "    gc.collect()\n",
    "    progress_bar = tqdm_notebook(valloader)\n",
    "    targets = np.zeros((len(val_target_filepaths), bev_shape[0], bev_shape[1]), dtype=np.uint8)\n",
    "    # We quantize to uint8 here to conserve memory. We're allocating >20GB of memory otherwise.\n",
    "    predictions = np.zeros((len(val_target_filepaths), 1+len(classes), bev_shape[0], \n",
    "                            bev_shape[1]), dtype=np.uint8)\n",
    "    sample_tokens = []\n",
    "    \n",
    "    ## predict\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for ii, (X, target, batch_sample_tokens) in enumerate(progress_bar):\n",
    "            offset = ii*batch_size\n",
    "            targets[offset:offset+batch_size] = target.numpy()\n",
    "            sample_tokens.extend(batch_sample_tokens)\n",
    "            X = X.to(device)  # [N, 1, H, W]\n",
    "            target = target.to(device)  # [N, H, W] with class indices (0, 1)\n",
    "            prediction = model(X)  # [N, 2, H, W]\n",
    "            prediction = F.softmax(prediction, dim=1)\n",
    "            prediction_cpu = prediction.cpu().numpy()\n",
    "            predictions[offset:offset+batch_size] = np.round(prediction_cpu*255).astype(np.uint8)\n",
    "    predictions_non_class0 = 255 - predictions[:,0]   \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
    "    predictions_opened = np.zeros((predictions_non_class0.shape), dtype=np.uint8)\n",
    "    background_threshold = 255//2\n",
    "    for i, p in enumerate(tqdm(predictions_non_class0)):\n",
    "        thresholded_p = (p > background_threshold).astype(np.uint8)\n",
    "        predictions_opened[i] = cv2.morphologyEx(thresholded_p, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    ## 2d detection\n",
    "    detection_boxes = []\n",
    "    detection_scores = []\n",
    "    detection_classes = []\n",
    "\n",
    "    for i in tqdm_notebook(range(len(predictions))):\n",
    "        prediction_opened = predictions_opened[i]\n",
    "        probability_non_class0 = predictions_non_class0[i]\n",
    "        class_probability = predictions[i]\n",
    "        sample_boxes = []\n",
    "        sample_detection_scores = []\n",
    "        sample_detection_classes = []\n",
    "        contours, hierarchy = cv2.findContours(prediction_opened, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)     \n",
    "        for cnt in contours:\n",
    "            rect = cv2.minAreaRect(cnt)\n",
    "            box = cv2.boxPoints(rect)        \n",
    "            # Let's take the center pixel value as the confidence value\n",
    "            box_center_index = np.int0(np.mean(box, axis=0))        \n",
    "            for class_index in range(len(classes)):\n",
    "                box_center_value = class_probability[class_index+1, box_center_index[1], box_center_index[0]]            \n",
    "                # Let's remove candidates with very low probability\n",
    "                if box_center_value < 0.01:\n",
    "                    continue            \n",
    "                box_center_class = classes[class_index]\n",
    "                box_detection_score = box_center_value\n",
    "                sample_detection_classes.append(box_center_class)\n",
    "                sample_detection_scores.append(box_detection_score)\n",
    "                sample_boxes.append(box)            \n",
    "        detection_boxes.append(np.array(sample_boxes))\n",
    "        detection_scores.append(sample_detection_scores)\n",
    "        detection_classes.append(sample_detection_classes)\n",
    "    \n",
    "    ## 3d detection\n",
    "    pred_box3ds = []\n",
    "    for (sample_token, sample_boxes, sample_detection_scores, sample_detection_class) in tqdm_notebook(\n",
    "            zip(sample_tokens, detection_boxes, detection_scores, \n",
    "            detection_classes), total=len(sample_tokens)):\n",
    "        sample_boxes = sample_boxes.reshape(-1, 2) # (N, 4, 2) -> (N*4, 2)\n",
    "        sample_boxes = sample_boxes.transpose(1,0) # (N*4, 2) -> (2, N*4)\n",
    "        # Add Z dimension\n",
    "        sample_boxes = np.vstack((sample_boxes, np.zeros(sample_boxes.shape[1]),)) # (2, N*4) -> (3, N*4)\n",
    "        sample = level5data.get(\"sample\", sample_token)\n",
    "        sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "        lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "        lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "        ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "        ego_translation = np.array(ego_pose['translation'])\n",
    "        global_from_car = transform_matrix(ego_pose['translation'],\n",
    "                                       Quaternion(ego_pose['rotation']), inverse=False)\n",
    "        car_from_voxel = np.linalg.inv(create_transformation_matrix_to_voxel_space(\n",
    "                                        bev_shape, voxel_size, (0, 0, z_offset)))\n",
    "        global_from_voxel = np.dot(global_from_car, car_from_voxel)\n",
    "        sample_boxes = transform_points(sample_boxes, global_from_voxel)\n",
    "        sample_boxes[2,:] = ego_pose[\"translation\"][2]\n",
    "        # (3, N*4) -> (N, 4, 3)\n",
    "        sample_boxes = sample_boxes.transpose(1,0).reshape(-1, 4, 3)\n",
    "        # We don't know the height of our boxes, let's assume every object is the same height.\n",
    "        box_height = 1.75\n",
    "        sample_boxes_centers = sample_boxes.mean(axis=1)\n",
    "        sample_boxes_centers[:,2] += box_height/2    \n",
    "        sample_lengths = np.linalg.norm(sample_boxes[:,0,:] - sample_boxes[:,1,:], axis=1) * 1/box_scale\n",
    "        sample_widths = np.linalg.norm(sample_boxes[:,1,:] - sample_boxes[:,2,:], axis=1) * 1/box_scale    \n",
    "        sample_boxes_dimensions = np.zeros_like(sample_boxes_centers) \n",
    "        sample_boxes_dimensions[:,0] = sample_widths\n",
    "        sample_boxes_dimensions[:,1] = sample_lengths\n",
    "        sample_boxes_dimensions[:,2] = box_height\n",
    "        for i in range(len(sample_boxes)):\n",
    "            translation = sample_boxes_centers[i]\n",
    "            size = sample_boxes_dimensions[i]\n",
    "            class_name = sample_detection_class[i]\n",
    "            ego_distance = float(np.linalg.norm(ego_translation - translation))          \n",
    "            # Determine the rotation of the box\n",
    "            v = (sample_boxes[i,0] - sample_boxes[i,1])\n",
    "            v /= np.linalg.norm(v)\n",
    "            r = R.from_dcm([ # initialization from direction cosine matrices\n",
    "                [v[0], -v[1], 0],\n",
    "                [v[1],  v[0], 0],\n",
    "                [   0,     0, 1],\n",
    "            ])\n",
    "            quat = r.as_quat() # as quaternions\n",
    "            # XYZW -> WXYZ order of elements\n",
    "            quat = quat[[3,0,1,2]]        \n",
    "            detection_score = float(sample_detection_scores[i])        \n",
    "            box3d = Box3D(\n",
    "                sample_token=sample_token,\n",
    "                translation=list(translation),\n",
    "                size=list(size),\n",
    "                rotation=list(quat),\n",
    "                name=class_name,\n",
    "                score=detection_score\n",
    "            )\n",
    "            pred_box3ds.append(box3d)\n",
    "    \n",
    "    ## mAP calc\n",
    "    pred = [b.serialize() for b in pred_box3ds]\n",
    "    os.makedirs(os.path.join(ARTIFACTS_FOLDER, \"json/resnet{}_{}\".format(reslayers, \n",
    "            resolution)), exist_ok=True)\n",
    "    with open(os.path.join(ARTIFACTS_FOLDER, \"json/resnet{}_{}/pred_{}.json\".format(reslayers, \n",
    "            resolution, epoch)), \"w\") as f:\n",
    "        json.dump(pred, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46939645e1304c228a48fe92d291879c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=630), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5040/5040 [00:01<00:00, 4307.75it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc25cce56f7d413eac79790d4d3b640d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5040), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7fb056a9ad4825853f594af5ba2587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5040), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38707e74a6a247d289a8747b3c36946e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=630), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5040/5040 [00:01<00:00, 4282.85it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145c92e597c14439925a0fc48da0ed34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5040), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df763661f06d4783be9306dde8e94652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5040), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4decd995cff14dc5816d49119c6dd671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=630), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5040/5040 [00:01<00:00, 4396.35it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa678279cd7e415d820a1fc01c4fb2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5040), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5174d145c754c2ca0d87c0c93b57cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5040), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_map(34, 496, 17)\n",
    "val_map(34, 496, 14)\n",
    "val_map(34, 496, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.11975817030258928\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.039912937576430364)\n",
      "('bus', 0.10288388972858527)\n",
      "('car', 0.5008256375777733)\n",
      "('motorcycle', 0.029779762104956333)\n",
      "('other_vehicle', 0.2468314778460151)\n",
      "('pedestrian', 0.017650038300566957)\n",
      "('truck', 0.020181619286386863)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/resnet34_496/pred_5.json' --gt \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.12389716664967979\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.03157909324444978)\n",
      "('bus', 0.09410808625968548)\n",
      "('car', 0.5227501821457663)\n",
      "('motorcycle', 0.02335367550589498)\n",
      "('other_vehicle', 0.28597740843624136)\n",
      "('pedestrian', 0.012112069587737893)\n",
      "('truck', 0.021296818017662526)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/resnet34_496/pred_6.json' --gt \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.11731166124413105\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.028445990272446327)\n",
      "('bus', 0.09677346727455668)\n",
      "('car', 0.5033989492688048)\n",
      "('motorcycle', 0.016592293827658637)\n",
      "('other_vehicle', 0.2644164046445354)\n",
      "('pedestrian', 0.008997785890750697)\n",
      "('truck', 0.019868398774295776)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/resnet34_496/pred_7.json' --gt \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.12073994251500897\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.027183279435158022)\n",
      "('bus', 0.09695469931046397)\n",
      "('car', 0.5094287971235998)\n",
      "('motorcycle', 0.013829826009779321)\n",
      "('other_vehicle', 0.28883483653176995)\n",
      "('pedestrian', 0.009378773083396295)\n",
      "('truck', 0.0203093286259044)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/resnet34_496/pred_8.json' --gt \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.11977386558904493\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.032493030542954035)\n",
      "('bus', 0.09590759105511631)\n",
      "('car', 0.5040280271581521)\n",
      "('motorcycle', 0.017063569849596217)\n",
      "('other_vehicle', 0.2762636129029292)\n",
      "('pedestrian', 0.010173404213630645)\n",
      "('truck', 0.022261688989980856)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/resnet34_496/pred_10.json' --gt \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.11959670156728441\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.02884378216006104)\n",
      "('bus', 0.09204634568923813)\n",
      "('car', 0.5020172672771814)\n",
      "('motorcycle', 0.018747390621877302)\n",
      "('other_vehicle', 0.28336968843247246)\n",
      "('pedestrian', 0.008571127518510194)\n",
      "('truck', 0.0231780108389349)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/resnet34_496/pred_11.json' --gt \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.11939690157026486\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.030799605272083856)\n",
      "('bus', 0.08972601582707765)\n",
      "('car', 0.49994655364088725)\n",
      "('motorcycle', 0.017197650275445852)\n",
      "('other_vehicle', 0.2875608951717036)\n",
      "('pedestrian', 0.0076383398205137065)\n",
      "('truck', 0.022306152554407092)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/resnet34_496/pred_17.json' --gt \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.12239122997635343\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.03502736081936683)\n",
      "('bus', 0.09732330180033058)\n",
      "('car', 0.5002790922148279)\n",
      "('motorcycle', 0.028800744038881387)\n",
      "('other_vehicle', 0.28735357619282836)\n",
      "('pedestrian', 0.008578418287515543)\n",
      "('truck', 0.02176734645707686)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/resnet34_496/pred_14.json' --gt \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_names =  ['animal', 'bicycle', 'bus', 'car', 'motorcycle', 'other_vehicle', 'pedestrian', 'truck']\n",
      "Average per class mean average precision =  0.11983471488778014\n",
      "('animal', 0.0)\n",
      "('bicycle', 0.03284659046339944)\n",
      "('bus', 0.09001153702322082)\n",
      "('car', 0.5021283554507802)\n",
      "('motorcycle', 0.017105834196256494)\n",
      "('other_vehicle', 0.28638917597916713)\n",
      "('pedestrian', 0.008422013272659592)\n",
      "('truck', 0.02177421271675741)\n"
     ]
    }
   ],
   "source": [
    "!python ./eval/mAP_evaluation.py --pred_file './artifacts/json/resnet34_496/pred_15.json' --gt \"./artifacts/gt.json\" --iou_threshold 0.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
